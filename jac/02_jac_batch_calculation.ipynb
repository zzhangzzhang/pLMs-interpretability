{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import esm\n",
    "import json \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from utils import *\n",
    "import random \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../data/selected_protein.json', 'r') as file:\n",
    "    selected_protein = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtRKmskxgHfs",
    "outputId": "b3ee2963-1147-4d6d-f6b0-31c0a41fafff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3rfMdHYpg1Yl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put model on GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "#model.args.token_dropout = False\n",
    "\n",
    "esm_alphabet = \"\".join(alphabet.all_toks[4:24])+\"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WWoCr2wujs5Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_logits(seq):\n",
    "  x,ln = alphabet.get_batch_converter()([(\"seq\",seq)])[-1],len(seq)\n",
    "  with torch.no_grad():\n",
    "    f = lambda x: model(x)[\"logits\"][0,1:(ln+1),4:24].cpu().numpy()\n",
    "    x = x.to(device)\n",
    "    logits = f(x)\n",
    "    return logits\n",
    "\n",
    "def get_masked_logits(seq, p=None, get_pll=False):\n",
    "  x,ln = alphabet.get_batch_converter()([(None,seq)])[-1],len(seq)\n",
    "  if p is None: p = ln\n",
    "  with torch.no_grad():\n",
    "    def f(x):\n",
    "      fx = model(x)[\"logits\"][:,1:(ln+1),4:24]\n",
    "      return fx\n",
    "\n",
    "    logits = np.zeros((ln,20))\n",
    "    for n in range(0,ln,p):\n",
    "      m = min(n+p,ln)\n",
    "      x_h = torch.tile(torch.clone(x),[m-n,1])\n",
    "      for i in range(m-n):\n",
    "        x_h[i,n+i+1] = alphabet.mask_idx\n",
    "      fx_h = f(x_h.to(device))\n",
    "      for i in range(m-n):\n",
    "        logits[n+i] = fx_h[i,n+i].cpu().numpy()\n",
    "  if get_pll:\n",
    "    logits = np.log(softmax(logits,-1))\n",
    "    x = x.cpu().numpy()[0]\n",
    "    x = x[1:(ln+1)] - 4\n",
    "    return sum([logits[n,i] for n,i in enumerate(x)])\n",
    "  else:\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wB0U1aeEmVn4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_categorical_jacobian(seq):\n",
    "  # ∂in/∂out\n",
    "  x,ln = alphabet.get_batch_converter()([(\"seq\",seq)])[-1],len(seq)\n",
    "  with torch.no_grad():\n",
    "    f = lambda x: model(x)[\"logits\"][...,1:(ln+1),4:24].cpu().numpy()\n",
    "    fx = f(x.to(device))[0]\n",
    "    x = torch.tile(x,[20,1]).to(device)\n",
    "    fx_h = np.zeros((ln,20,ln,20))\n",
    "    for n in range(ln): # for each position\n",
    "      x_h = torch.clone(x)\n",
    "      x_h[:,n+1] = torch.arange(4,24) # mutate to all 20 aa\n",
    "      fx_h[n] = f(x_h)\n",
    "    return fx-fx_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(pdb): \n",
    "    file_path = 'msa/' + pdb + '.a3m'\n",
    "    \n",
    "    headers, seqs = parse_fasta(file_path, a3m = True)\n",
    "    msa = mk_msa(seqs, alphabet=esm_alphabet)\n",
    "    seq = seqs[0]\n",
    "    \n",
    "    tmp = inv_cov_jax(msa)\n",
    "    print(tmp['apc'].shape)\n",
    "    with open('../data/inv_cov_msa/' + pdb + '_inv_cov_msa.pkl', 'wb') as f:\n",
    "        pickle.dump(tmp, f)\n",
    "    \n",
    "    with open('../data/msa_contact/' + pdb + '_msa_contact.pkl', 'wb') as f:\n",
    "        pickle.dump(tmp[\"apc\"], f)\n",
    "    \n",
    "    # jacobian of the model\n",
    "    jac = get_categorical_jacobian(seq)\n",
    "    # center & symmetrize\n",
    "    for i in range(4): jac -= jac.mean(i,keepdims=True)\n",
    "    jac = (jac + jac.transpose(2,3,0,1))/2\n",
    "    print(jac.shape)\n",
    "    \n",
    "    with open('../results/esm2_jac/' + pdb + '_esm2_jac.pkl', 'wb') as f:\n",
    "        pickle.dump(jac, f)\n",
    "    \n",
    "    jac_contacts = get_contacts(jac)\n",
    "    with open('../results/esm2_jac_contact/' + pdb + '_esm2_jac_contact.pkl', 'wb') as f:\n",
    "        pickle.dump(jac_contacts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/237 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4M2MA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "for pdb in tqdm(selected_protein):\n",
    "    get_data(pdb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMtUIoA0J1OPju2AsfFC58O",
   "include_colab_link": true,
   "name": "BERT_esm1b.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "so_covar",
   "language": "python",
   "name": "so_covar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
